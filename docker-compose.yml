version: "3.1"

services:
  flowise:
    image: elestio/flowiseai
    platform: linux/arm64
    restart: always
    environment:
      - PORT=3000
      # - FLOWISE_USERNAME=${USERNAME}
      # - FLOWISE_PASSWORD=${ADMIN_PASSWORD}
    ports:
      - "127.0.0.1:3000:3000"
    volumes:
      - ./data/flowise/.flowise:/root/.flowise

  api:
    # See https://localai.io/basics/getting_started/#container-images for
    # a list of available container images (or build your own with the provided Dockerfile)
    # Available images with CUDA, ROCm, SYCL
    # Image list (quay.io): https://quay.io/repository/go-skynet/local-ai?tab=tags
    # Image list (dockerhub): https://hub.docker.com/r/localai/localai
    image: quay.io/go-skynet/local-ai:master-ffmpeg-core
    build:
      context: .
      dockerfile: Dockerfile
      args:
        - IMAGE_TYPE=core
        - BASE_IMAGE=ubuntu:22.04
    ports:
      - 8080:8080
    env_file:
      - .env
    environment:
      - MODELS_PATH=/models
    #  - DEBUG=true
    volumes:
      - ./models:/models:cached
      - ./images/:/tmp/generated/images/
    command:
      # Here we can specify a list of models to run (see quickstart https://localai.io/basics/getting_started/#running-models )
      # or an URL pointing to a YAML configuration file, for example:
      # - https://gist.githubusercontent.com/mudler/ad601a0488b497b69ec549150d9edd18/raw/a8a8869ef1bb7e3830bf5c0bae29a0cce991ff8d/phi-2.yaml
      - phi-2

  #api:
    # image: localai/localai:latest-aio-cpu
    # For a specific version:
    # image: localai/localai:v2.20.1-aio-cpu
    # For Nvidia GPUs decomment one of the following (cuda11 or cuda12):
    # image: localai/localai:v2.20.1-aio-gpu-nvidia-cuda-11
    # image: localai/localai:v2.20.1-aio-gpu-nvidia-cuda-12
    # image: localai/localai:latest-aio-gpu-nvidia-cuda-11
    #image: localai/localai:${LOCALAI_VERSION}
    #platform: linux/arm64
    # image: localai/localai:latest-aio-gpu-nvidia-cuda-12
    # deploy:
    #   resources:
    #     reservations:
    #       devices:
    #         - driver: nvidia
    #           count: 1
    #           capabilities: [gpu]
    #healthcheck:
     # test: ["CMD", "curl", "-f", "http://localhost:8989/readyz"]
      #interval: 1m
      #timeout: 20m
      #retries: 5
    #ports:
     # - 127.0.0.1:8989:8989
    #environment:
     # - DEBUG=true
      # ...
    #volumes:
     # - ./data/localai/models:/build/models:cached
      # - ./data/localai/models:/build/models